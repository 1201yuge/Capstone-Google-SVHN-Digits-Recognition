{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle\n",
    "from six.moves import range\n",
    "import scipy.io   \n",
    "import h5py\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import tarfile\n",
    "from IPython.display import display, Image\n",
    "\n",
    "from scipy import ndimage\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (27401, 32, 32, 1) (27401, 6)\n",
      "Validation set (6000, 32, 32, 1) (6000, 6)\n",
      "Test set (13068, 32, 32, 1) (13068, 6)\n"
     ]
    }
   ],
   "source": [
    "pickle_file = 'SVHN_multi_crop.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "    save = pickle.load(f)\n",
    "    train_dataset = save['train_dataset']\n",
    "    train_labels = save['train_labels']\n",
    "    valid_dataset = save['valid_dataset']\n",
    "    valid_labels = save['valid_labels']\n",
    "    test_dataset = save['test_dataset']\n",
    "    test_labels = save['test_labels']\n",
    "    del save  # hint to help gc free up memory\n",
    "    print('Training set', train_dataset.shape, train_labels.shape)\n",
    "    print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "    print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "patch_size = 5\n",
    "depth1 = 16\n",
    "depth2 = 32\n",
    "depth3 = 64\n",
    "num_hidden = 1024\n",
    "\n",
    "image_size = 32\n",
    "num_labels = 11 # 0-9, + blank \n",
    "num_channels = 1 # grayscale\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "\n",
    "    def weight_varible(shape):\n",
    "        initial = tf.truncated_normal(shape, stddev = 0.1)\n",
    "        return tf.Variable(initial)\n",
    "\n",
    "    def bias_varible(shape):\n",
    "        initial = tf.constant(1.0, shape = shape)\n",
    "        return tf.Variable(initial)\n",
    "    \n",
    "    def conv2d(data, weight):\n",
    "        # strides [1, x_movement, y_movement, 1]\n",
    "        return tf.nn.conv2d(data, weight, strides = [1, 1, 1, 1], padding = 'SAME')\n",
    "\n",
    "    def max_pooling(data):\n",
    "        return tf.nn.max_pool(data, ksize = [1, 2, 2, 1], strides = [1, 2, 2, 1], padding = 'SAME')\n",
    "    \n",
    "    def get_class_wb():\n",
    "        weights = tf.Variable(tf.truncated_normal([num_hidden, num_labels]))\n",
    "        biases = tf.Variable(tf.constant(1.0, shape=[num_labels]))\n",
    "#         logits = tf.matmul(tf_train_dataset, weights) + biases\n",
    "        return weights, biases #, logits\n",
    "\n",
    "    \n",
    "    # Input data.\n",
    "    tf_train_dataset = tf.placeholder(\n",
    "    tf.float32, shape=(batch_size, image_size, image_size, num_channels))\n",
    "    tf_train_labels = tf.placeholder(tf.int32, shape=(batch_size, 6))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    \n",
    "    \n",
    "    # Varibles\n",
    "    # conv1 layer 1\n",
    "    layer1_weights = weight_varible([patch_size, patch_size, num_channels, depth1])\n",
    "    layer1_biases = bias_varible([depth1]) # 16\n",
    "    # conv2 layer 2\n",
    "    layer2_weights = weight_varible([patch_size, patch_size, depth1, depth2]) # in depth1, out depth2\n",
    "    layer2_biases = bias_varible([depth2]) # 32\n",
    "    # conv3 layer 3\n",
    "    layer3_weights = weight_varible([patch_size, patch_size, depth2, depth3]) # in depth2, out depth3\n",
    "    layer3_biases = bias_varible([depth3]) # 64\n",
    "    \n",
    "    \n",
    "    s1_w, s1_b = get_class_wb()\n",
    "    s2_w, s2_b = get_class_wb()\n",
    "    s3_w, s3_b = get_class_wb()\n",
    "    s4_w, s4_b = get_class_wb()\n",
    "    s5_w, s5_b = get_class_wb()\n",
    "    \n",
    "#     # func1 layer 4\n",
    "#     layer3_weights = weight_varible([image_size // 8 * image_size // 8 * depth3, num_hidden])\n",
    "#     layer3_biases = bias_varible([num_hidden])\n",
    "#     # func2 layer 5\n",
    "#     layer4_weights = weight_varible([num_hidden, num_labels])\n",
    "#     layer4_biases = bias_varible([num_labels])\n",
    "    \n",
    "    global_step = tf.Variable(0)  # count the number of steps taken.\n",
    "    \n",
    "    def model(dataset):\n",
    "        \n",
    "        # conv1 layer 1\n",
    "        hidden1 = tf.nn.relu(conv2d(dataset, layer1_weights) + layer1_biases) # 32 * 32 * depth1\n",
    "        pool1 = max_pooling(hidden1) # 16 * 16 * depth1\n",
    "        # conv2 layer 2\n",
    "        hidden2 = tf.nn.relu(conv2d(pool1, layer2_weights) + layer2_biases) # 16 * 16 * depth2\n",
    "        pool2 = max_pooling(hidden2) # 8 * 8 * depth2\n",
    "        # conv3 layer 3\n",
    "        hidden3 = tf.nn.relu(conv2d(pool2, layer3_weights) + layer3_biases) # 8 * 8 * depth3\n",
    "        pool3 = max_pooling(hidden3) # 4 * 4 * depth3\n",
    "        \n",
    "        shape = pool3.get_shape().as_list()\n",
    "        pool3_flat = tf.reshape(pool3, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "\n",
    "#         print([shape[0], shape[1] * shape[2] * shape[3]])\n",
    "#         print([image_size // 4 * image_size // 4 * depth2, num_hidden])\n",
    "\n",
    "        logits_1 = tf.matmul(pool3_flat, s1_w) + s1_b\n",
    "        logits_2 = tf.matmul(pool3_flat, s2_w) + s2_b\n",
    "        logits_3 = tf.matmul(pool3_flat, s3_w) + s3_b\n",
    "        logits_4 = tf.matmul(pool3_flat, s4_w) + s4_b\n",
    "        logits_5 = tf.matmul(pool3_flat, s5_w) + s5_b\n",
    "    \n",
    "        \n",
    "#         # func1 layer 4\n",
    "#         hidden4 = tf.nn.relu(tf.matmul(pool3_flat, layer4_weights) + layer4_biases)\n",
    "#         hidden4_drop = tf.nn.dropout(hidden3, 0.5)\n",
    "#         # func2 layer 5\n",
    "#         prediction = tf.matmul(hidden4_drop, layer5_weights) + layer5_biases\n",
    "        \n",
    "        \n",
    "        return [logits_1, logits_2, logits_3, logits_4, logits_5]\n",
    "    \n",
    "# Training computation.\n",
    "    [logits_1, logits_2, logits_3, logits_4, logits_5] = model(tf_train_dataset)\n",
    "    \n",
    "    loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits_1, tf_train_labels[:,1])) +\\\n",
    "tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits_2, tf_train_labels[:,2])) +\\\n",
    "tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits_3, tf_train_labels[:,3])) +\\\n",
    "tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits_4, tf_train_labels[:,4])) +\\\n",
    "tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits_5, tf_train_labels[:,5]))\n",
    "\n",
    "#     loss = tf.reduce_mean(\n",
    "#     tf.nn.softmax_cross_entropy_with_logits(all_logits, tf_train_labels))\n",
    "    \n",
    "    # Optimizer.\n",
    "\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.05).minimize(loss, global_step=global_step)\n",
    "\n",
    "    # Predictions for the training, validation, and test data.\n",
    "#     train_prediction = tf.nn.softmax(logits_1)\n",
    "#     valid_prediction = tf.nn.softmax(model(tf_valid_dataset))\n",
    "#     test_prediction = tf.nn.softmax(model(tf_test_dataset))\n",
    "    \n",
    "    \n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.pack([tf.nn.softmax(logits_1),\\\n",
    "                      tf.nn.softmax(logits_2),\\\n",
    "                      tf.nn.softmax(logits_3),\\\n",
    "                      tf.nn.softmax(logits_4),\\\n",
    "                      tf.nn.softmax(logits_5)])\n",
    "    valid_prediction = tf.pack([tf.nn.softmax(model(tf_valid_dataset)[0]),\\\n",
    "                      tf.nn.softmax(model(tf_valid_dataset)[1]),\\\n",
    "                      tf.nn.softmax(model(tf_valid_dataset)[2]),\\\n",
    "                      tf.nn.softmax(model(tf_valid_dataset)[3]),\\\n",
    "                      tf.nn.softmax(model(tf_valid_dataset)[4])])\n",
    "    test_prediction = tf.pack([tf.nn.softmax(model(tf_test_dataset)[0]),\\\n",
    "                     tf.nn.softmax(model(tf_test_dataset)[1]),\\\n",
    "                     tf.nn.softmax(model(tf_test_dataset)[2]),\\\n",
    "                     tf.nn.softmax(model(tf_test_dataset)[3]),\\\n",
    "                     tf.nn.softmax(model(tf_test_dataset)[4])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def accuracy(predictions, labels):\n",
    "#     return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "#           / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "    return (100.0 * np.sum(np.argmax(predictions, 2).T == labels) / predictions.shape[1] / predictions.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 1079.544678\n",
      "Minibatch accuracy: 1.6%\n",
      "Validation accuracy: 59.2%\n",
      "Minibatch loss at step 500: 5.537723\n",
      "Minibatch accuracy: 66.4%\n",
      "Validation accuracy: 64.5%\n",
      "Minibatch loss at step 1000: 5.493402\n",
      "Minibatch accuracy: 67.0%\n",
      "Validation accuracy: 64.5%\n",
      "Minibatch loss at step 1500: 5.615956\n",
      "Minibatch accuracy: 63.6%\n",
      "Validation accuracy: 64.5%\n",
      "Minibatch loss at step 2000: 5.762838\n",
      "Minibatch accuracy: 63.8%\n",
      "Validation accuracy: 64.5%\n",
      "Minibatch loss at step 2500: 5.474291\n",
      "Minibatch accuracy: 66.6%\n",
      "Validation accuracy: 64.5%\n",
      "Minibatch loss at step 3000: 5.868134\n",
      "Minibatch accuracy: 62.8%\n",
      "Validation accuracy: 64.5%\n",
      "Minibatch loss at step 3500: 5.542148\n",
      "Minibatch accuracy: 65.2%\n",
      "Validation accuracy: 64.5%\n",
      "Minibatch loss at step 4000: 5.684011\n",
      "Minibatch accuracy: 63.8%\n",
      "Validation accuracy: 64.5%\n",
      "Minibatch loss at step 4500: 5.375666\n",
      "Minibatch accuracy: 65.5%\n",
      "Validation accuracy: 64.5%\n",
      "Minibatch loss at step 5000: 5.537566\n",
      "Minibatch accuracy: 64.7%\n",
      "Validation accuracy: 64.5%\n",
      "Minibatch loss at step 5500: 5.668209\n",
      "Minibatch accuracy: 64.5%\n",
      "Validation accuracy: 64.5%\n",
      "Minibatch loss at step 6000: 5.607848\n",
      "Minibatch accuracy: 63.3%\n",
      "Validation accuracy: 64.5%\n",
      "Minibatch loss at step 6500: 5.650269\n",
      "Minibatch accuracy: 65.3%\n",
      "Validation accuracy: 64.5%\n",
      "Minibatch loss at step 7000: 5.871630\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy: 64.5%\n",
      "Minibatch loss at step 7500: 5.900801\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy: 64.5%\n",
      "Minibatch loss at step 8000: 5.749823\n",
      "Minibatch accuracy: 64.2%\n",
      "Validation accuracy: 64.5%\n",
      "Minibatch loss at step 8500: 5.386055\n",
      "Minibatch accuracy: 65.6%\n",
      "Validation accuracy: 64.5%\n",
      "Minibatch loss at step 9000: 5.542713\n",
      "Minibatch accuracy: 65.0%\n",
      "Validation accuracy: 64.5%\n",
      "Minibatch loss at step 9500: 5.152781\n",
      "Minibatch accuracy: 67.7%\n",
      "Validation accuracy: 64.5%\n",
      "Minibatch loss at step 10000: 5.950256\n",
      "Minibatch accuracy: 63.0%\n",
      "Validation accuracy: 64.5%\n",
      "Minibatch loss at step 10500: 5.613161\n",
      "Minibatch accuracy: 64.5%\n",
      "Validation accuracy: 64.5%\n",
      "Minibatch loss at step 11000: 5.762372\n",
      "Minibatch accuracy: 62.8%\n",
      "Validation accuracy: 64.5%\n",
      "Minibatch loss at step 11500: 5.690709\n",
      "Minibatch accuracy: 64.7%\n",
      "Validation accuracy: 64.5%\n",
      "Minibatch loss at step 12000: 5.593603\n",
      "Minibatch accuracy: 64.5%\n",
      "Validation accuracy: 64.5%\n",
      "Minibatch loss at step 12500: 5.666915\n",
      "Minibatch accuracy: 64.2%\n",
      "Validation accuracy: 64.5%\n",
      "Minibatch loss at step 13000: 5.829227\n",
      "Minibatch accuracy: 64.5%\n",
      "Validation accuracy: 64.5%\n",
      "Minibatch loss at step 13500: 5.676597\n",
      "Minibatch accuracy: 63.3%\n",
      "Validation accuracy: 64.5%\n",
      "Minibatch loss at step 14000: 5.591484\n",
      "Minibatch accuracy: 65.8%\n",
      "Validation accuracy: 64.5%\n",
      "Minibatch loss at step 14500: 5.551265\n",
      "Minibatch accuracy: 64.7%\n",
      "Validation accuracy: 64.5%\n",
      "Minibatch loss at step 15000: 5.814839\n",
      "Minibatch accuracy: 63.8%\n",
      "Validation accuracy: 64.5%\n",
      "Minibatch loss at step 15500: 5.471779\n",
      "Minibatch accuracy: 66.2%\n",
      "Validation accuracy: 64.5%\n",
      "Minibatch loss at step 16000: 5.413076\n",
      "Minibatch accuracy: 68.1%\n",
      "Validation accuracy: 64.5%\n",
      "Minibatch loss at step 16500: 5.469202\n",
      "Minibatch accuracy: 66.1%\n",
      "Validation accuracy: 64.5%\n",
      "Minibatch loss at step 17000: 5.627341\n",
      "Minibatch accuracy: 64.1%\n",
      "Validation accuracy: 64.5%\n",
      "Minibatch loss at step 17500: 5.413357\n",
      "Minibatch accuracy: 65.5%\n",
      "Validation accuracy: 64.5%\n",
      "Minibatch loss at step 18000: 5.561985\n",
      "Minibatch accuracy: 65.0%\n",
      "Validation accuracy: 64.5%\n",
      "Minibatch loss at step 18500: 5.530137\n",
      "Minibatch accuracy: 64.7%\n",
      "Validation accuracy: 64.5%\n",
      "Minibatch loss at step 19000: 5.867750\n",
      "Minibatch accuracy: 63.3%\n",
      "Validation accuracy: 64.5%\n",
      "Minibatch loss at step 19500: 6.023471\n",
      "Minibatch accuracy: 63.8%\n",
      "Validation accuracy: 64.5%\n",
      "Minibatch loss at step 20000: 5.510715\n",
      "Minibatch accuracy: 64.8%\n",
      "Validation accuracy: 64.5%\n",
      "Test accuracy: 68.5%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 20001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "#     writer = tf.train.SummaryWriter(\"logs/\", session.graph)\n",
    "    tf.initialize_all_variables().run()\n",
    "    print('Initialized')\n",
    "    for step in range(num_steps):\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :, :, :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "        _, l, predictions = session.run(\n",
    "          [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        if (step % 500 == 0):\n",
    "            print('Minibatch loss at step %d: %f' % (step, l))\n",
    "            print('Minibatch accuracy: %.1f%%' % accuracy(predictions, batch_labels[:,1:6]))\n",
    "#             print(np.argmax(predictions, 1), np.argmax(batch_labels[:,1:6],1))\n",
    "            print('Validation accuracy: %.1f%%' % accuracy(\n",
    "            valid_prediction.eval(), valid_labels[:,1:6]))\n",
    "    \n",
    "    print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels[:,1:6]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "patch_size = 5\n",
    "depth1 = 16\n",
    "depth2 = 32\n",
    "depth3 = 64\n",
    "num_hidden = 64\n",
    "\n",
    "image_size = 32\n",
    "num_labels = 11 # 0-9, + blank \n",
    "num_channels = 1 # grayscale\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "\n",
    "    def weight_varible(shape):\n",
    "        initial = tf.truncated_normal(shape, stddev = 0.1)\n",
    "        return tf.Variable(initial)\n",
    "\n",
    "    def bias_varible(shape):\n",
    "        initial = tf.constant(1.0, shape = shape)\n",
    "        return tf.Variable(initial)\n",
    "    \n",
    "    def conv2d(data, weight):\n",
    "        # strides [1, x_movement, y_movement, 1]\n",
    "        return tf.nn.conv2d(data, weight, strides = [1, 1, 1, 1], padding = 'VALID')\n",
    "\n",
    "    def max_pooling(data):\n",
    "        return tf.nn.max_pool(data, ksize = [1, 2, 2, 1], strides = [1, 2, 2, 1], padding = 'SAME')\n",
    "    \n",
    "#     def get_class_wb():\n",
    "#         weights = tf.Variable(tf.truncated_normal([num_hidden, num_labels]))\n",
    "#         biases = tf.Variable(tf.constant(1.0, shape=[num_labels]))\n",
    "#         return weights, biases #, logits\n",
    "    \n",
    "    # Input data.\n",
    "    tf_train_dataset = tf.placeholder(\n",
    "    tf.float32, shape=(batch_size, image_size, image_size, num_channels))\n",
    "    tf_train_labels = tf.placeholder(tf.int32, shape=(batch_size, 6))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    beta_regul = tf.placeholder(tf.float32)\n",
    "    \n",
    "    \n",
    "    # Varibles\n",
    "    # conv1 layer 1\n",
    "#     layer1_weights = weight_varible([patch_size, patch_size, num_channels, depth1])\n",
    "    layer1_weights = tf.get_variable('W1',shape=[patch_size, patch_size, num_channels, depth1],\\\n",
    "                                     initializer=tf.contrib.layers.xavier_initializer_conv2d())\n",
    "    layer1_biases = bias_varible([depth1]) # 16\n",
    "    # conv2 layer 2\n",
    "#     layer2_weights = weight_varible([patch_size, patch_size, depth1, depth2]) # in depth1, out depth2\n",
    "    layer2_weights = tf.get_variable('W2',shape=[patch_size, patch_size, depth1, depth2],\\\n",
    "                                     initializer=tf.contrib.layers.xavier_initializer_conv2d())\n",
    "    layer2_biases = bias_varible([depth2]) # 32\n",
    "    # conv3 layer 3\n",
    "#     layer3_weights = weight_varible([patch_size, patch_size, depth2, depth3]) # in depth2, out depth3\n",
    "    layer3_weights = tf.get_variable('W3',shape=[patch_size, patch_size, depth2, depth3],\\\n",
    "                                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "    layer3_biases = bias_varible([depth3]) # 64\n",
    "    \n",
    "    # func1 layer 4\n",
    "#     layer4_weights = weight_varible([image_size // 32 * image_size // 32 * depth3, num_hidden])\n",
    "#     layer4_weights = tf.get_variable('W4',shape=[image_size // 32 * image_size // 32 * depth3, num_hidden],\\\n",
    "#                                      initializer=tf.contrib.layers.xavier_initializer_conv2d())\n",
    "#     layer4_biases = bias_varible([num_hidden])\n",
    "\n",
    "    s1_w = tf.get_variable(\"WS1\", shape=[num_hidden, num_labels],\\\n",
    "           initializer=tf.contrib.layers.xavier_initializer())\n",
    "    s1_b = tf.Variable(tf.constant(1.0, shape=[num_labels]), name='BS1')\n",
    "    s2_w = tf.get_variable(\"WS2\", shape=[num_hidden, num_labels],\\\n",
    "           initializer=tf.contrib.layers.xavier_initializer())\n",
    "    s2_b = tf.Variable(tf.constant(1.0, shape=[num_labels]), name='BS2')\n",
    "    s3_w = tf.get_variable(\"WS3\", shape=[num_hidden, num_labels],\\\n",
    "           initializer=tf.contrib.layers.xavier_initializer())\n",
    "    s3_b = tf.Variable(tf.constant(1.0, shape=[num_labels]), name='BS3')\n",
    "    s4_w = tf.get_variable(\"WS4\", shape=[num_hidden, num_labels],\\\n",
    "           initializer=tf.contrib.layers.xavier_initializer())\n",
    "    s4_b = tf.Variable(tf.constant(1.0, shape=[num_labels]), name='BS4')\n",
    "    s5_w = tf.get_variable(\"WS5\", shape=[num_hidden, num_labels],\\\n",
    "           initializer=tf.contrib.layers.xavier_initializer())\n",
    "    s5_b = tf.Variable(tf.constant(1.0, shape=[num_labels]), name='BS5')\n",
    "    \n",
    "#     s1_w, s1_b = get_class_wb()\n",
    "#     s2_w, s2_b = get_class_wb()\n",
    "#     s3_w, s3_b = get_class_wb()\n",
    "#     s4_w, s4_b = get_class_wb()\n",
    "#     s5_w, s5_b = get_class_wb()\n",
    "    sw = [s1_w, s2_w, s3_w, s4_w, s5_w]\n",
    "    \n",
    "    global_step = tf.Variable(0)  # count the number of steps taken.\n",
    "    \n",
    "    def model(dataset):\n",
    "        # conv1 layer 1\n",
    "        hidden1 = tf.nn.relu(conv2d(dataset, layer1_weights) + layer1_biases) # 28 * 28 * depth1\n",
    "#         hidden1 = tf.nn.local_response_normalization(hidden1)\n",
    "        pool1 = max_pooling(hidden1) # 14 * 14 * depth1\n",
    "        # conv2 layer 2\n",
    "        hidden2 = tf.nn.relu(conv2d(pool1, layer2_weights) + layer2_biases) # 10 * 10 * depth2\n",
    "#         hidden2 = tf.nn.local_response_normalization(hidden2)\n",
    "        pool2 = max_pooling(hidden2) # 5 * 5 * depth2\n",
    "        # conv3 layer 3\n",
    "        pool3 = tf.nn.relu(conv2d(pool2, layer3_weights) + layer3_biases) # 1 * 1 * depth3\n",
    "#         pool3 = max_pooling(hidden3) # 1 * 1 * depth3\n",
    "        \n",
    "        shape = pool3.get_shape().as_list()\n",
    "        pool3_flat = tf.reshape(pool3, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "        \n",
    "#         # func1 layer 4\n",
    "#         hidden4 = tf.nn.relu(tf.matmul(pool3_flat, layer4_weights) + layer4_biases)\n",
    "        hidden4_drop = tf.nn.dropout(pool3_flat, 0.9375)\n",
    "\n",
    "        logits_1 = tf.matmul(hidden4_drop, s1_w) + s1_b\n",
    "        logits_2 = tf.matmul(hidden4_drop, s2_w) + s2_b\n",
    "        logits_3 = tf.matmul(hidden4_drop, s3_w) + s3_b\n",
    "        logits_4 = tf.matmul(hidden4_drop, s4_w) + s4_b\n",
    "        logits_5 = tf.matmul(hidden4_drop, s5_w) + s5_b\n",
    "        \n",
    "        return [logits_1, logits_2, logits_3, logits_4, logits_5]\n",
    "    \n",
    "# Training computation.\n",
    "#     logits = model(tf_train_dataset)\n",
    "    [logits_1, logits_2, logits_3, logits_4, logits_5] = model(tf_train_dataset)\n",
    "    \n",
    "#     loss_per_digit = [tf.reduce_mean(\n",
    "#                         tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "#                             logits[i],\n",
    "#                             tf_train_labels[:,i+1]\n",
    "#                         )) + beta_regul * tf.nn.l2_loss(sw[i])\n",
    "#                        for i in range(5)]\n",
    "    \n",
    "#     loss = tf.add_n(loss_per_digit)\n",
    "    loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits_1, tf_train_labels[:,1])) +\\\n",
    "tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits_2, tf_train_labels[:,2])) +\\\n",
    "tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits_3, tf_train_labels[:,3])) +\\\n",
    "tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits_4, tf_train_labels[:,4])) +\\\n",
    "tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits_5, tf_train_labels[:,5]))\n",
    "\n",
    "\n",
    "    \n",
    "    # Optimizer.\n",
    "    learning_rate = tf.train.exponential_decay(0.05, global_step, 1000, 0.70, staircase=True)\n",
    "    optimizer = tf.train.AdagradOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "    \n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.pack([tf.nn.softmax(model(tf_train_dataset)[0]),\\\n",
    "                      tf.nn.softmax(model(tf_train_dataset)[1]),\\\n",
    "                      tf.nn.softmax(model(tf_train_dataset)[2]),\\\n",
    "                      tf.nn.softmax(model(tf_train_dataset)[3]),\\\n",
    "                      tf.nn.softmax(model(tf_train_dataset)[4])])\n",
    "#     train_prediction = tf.pack([tf.nn.softmax(logits[i]) for i in range(5)])\n",
    "    valid_prediction = tf.pack([tf.nn.softmax(model(tf_valid_dataset)[0]),\\\n",
    "                      tf.nn.softmax(model(tf_valid_dataset)[1]),\\\n",
    "                      tf.nn.softmax(model(tf_valid_dataset)[2]),\\\n",
    "                      tf.nn.softmax(model(tf_valid_dataset)[3]),\\\n",
    "                      tf.nn.softmax(model(tf_valid_dataset)[4])])\n",
    "    test_prediction = tf.pack([tf.nn.softmax(model(tf_test_dataset)[0]),\\\n",
    "                     tf.nn.softmax(model(tf_test_dataset)[1]),\\\n",
    "                     tf.nn.softmax(model(tf_test_dataset)[2]),\\\n",
    "                     tf.nn.softmax(model(tf_test_dataset)[3]),\\\n",
    "                     tf.nn.softmax(model(tf_test_dataset)[4])])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 21.555344\n",
      "Minibatch accuracy: 9.7%\n",
      "Validation accuracy: 55.7%\n",
      "Minibatch loss at step 500: 5.558725\n",
      "Minibatch accuracy: 61.9%\n",
      "Validation accuracy: 65.6%\n",
      "Minibatch loss at step 1000: 4.721919\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 70.8%\n",
      "Minibatch loss at step 1500: 3.649049\n",
      "Minibatch accuracy: 78.8%\n",
      "Validation accuracy: 73.8%\n",
      "Minibatch loss at step 2000: 4.099685\n",
      "Minibatch accuracy: 75.3%\n",
      "Validation accuracy: 75.9%\n",
      "Minibatch loss at step 2500: 3.093337\n",
      "Minibatch accuracy: 80.3%\n",
      "Validation accuracy: 76.6%\n",
      "Minibatch loss at step 3000: 3.714985\n",
      "Minibatch accuracy: 78.1%\n",
      "Validation accuracy: 77.0%\n",
      "Minibatch loss at step 3500: 3.654895\n",
      "Minibatch accuracy: 75.6%\n",
      "Validation accuracy: 77.4%\n",
      "Minibatch loss at step 4000: 2.732865\n",
      "Minibatch accuracy: 85.3%\n",
      "Validation accuracy: 77.7%\n",
      "Minibatch loss at step 4500: 3.464983\n",
      "Minibatch accuracy: 79.1%\n",
      "Validation accuracy: 78.0%\n",
      "Minibatch loss at step 5000: 3.847943\n",
      "Minibatch accuracy: 77.8%\n",
      "Validation accuracy: 78.0%\n",
      "Minibatch loss at step 5500: 2.880893\n",
      "Minibatch accuracy: 81.9%\n",
      "Validation accuracy: 78.1%\n",
      "Minibatch loss at step 6000: 3.147345\n",
      "Minibatch accuracy: 80.6%\n",
      "Validation accuracy: 77.9%\n",
      "Minibatch loss at step 6500: 2.511845\n",
      "Minibatch accuracy: 84.7%\n",
      "Validation accuracy: 78.2%\n",
      "Minibatch loss at step 7000: 4.198817\n",
      "Minibatch accuracy: 72.8%\n",
      "Validation accuracy: 78.4%\n",
      "Minibatch loss at step 7500: 3.675305\n",
      "Minibatch accuracy: 78.1%\n",
      "Validation accuracy: 78.4%\n",
      "Minibatch loss at step 8000: 3.346276\n",
      "Minibatch accuracy: 80.6%\n",
      "Validation accuracy: 78.4%\n",
      "Minibatch loss at step 8500: 3.587855\n",
      "Minibatch accuracy: 76.2%\n",
      "Validation accuracy: 78.4%\n",
      "Minibatch loss at step 9000: 4.383076\n",
      "Minibatch accuracy: 76.2%\n",
      "Validation accuracy: 78.5%\n",
      "Minibatch loss at step 9500: 2.839370\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 78.5%\n",
      "Minibatch loss at step 10000: 3.454652\n",
      "Minibatch accuracy: 78.1%\n",
      "Validation accuracy: 78.4%\n",
      "Minibatch loss at step 10500: 4.322999\n",
      "Minibatch accuracy: 72.8%\n",
      "Validation accuracy: 78.5%\n",
      "Minibatch loss at step 11000: 3.585159\n",
      "Minibatch accuracy: 77.2%\n",
      "Validation accuracy: 78.5%\n",
      "Minibatch loss at step 11500: 2.547016\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 78.5%\n",
      "Minibatch loss at step 12000: 3.020973\n",
      "Minibatch accuracy: 81.9%\n",
      "Validation accuracy: 78.5%\n",
      "Minibatch loss at step 12500: 3.262986\n",
      "Minibatch accuracy: 81.9%\n",
      "Validation accuracy: 78.5%\n",
      "Minibatch loss at step 13000: 3.341381\n",
      "Minibatch accuracy: 79.7%\n",
      "Validation accuracy: 78.6%\n",
      "Minibatch loss at step 13500: 3.296968\n",
      "Minibatch accuracy: 78.8%\n",
      "Validation accuracy: 78.5%\n",
      "Minibatch loss at step 14000: 3.062422\n",
      "Minibatch accuracy: 78.8%\n",
      "Validation accuracy: 78.5%\n",
      "Minibatch loss at step 14500: 3.497693\n",
      "Minibatch accuracy: 77.5%\n",
      "Validation accuracy: 78.5%\n",
      "Minibatch loss at step 15000: 3.025767\n",
      "Minibatch accuracy: 80.6%\n",
      "Validation accuracy: 78.6%\n",
      "Minibatch loss at step 15500: 3.530257\n",
      "Minibatch accuracy: 80.3%\n",
      "Validation accuracy: 78.5%\n",
      "Minibatch loss at step 16000: 3.009552\n",
      "Minibatch accuracy: 81.6%\n",
      "Validation accuracy: 78.4%\n",
      "Minibatch loss at step 16500: 2.701258\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 78.4%\n",
      "Minibatch loss at step 17000: 3.004794\n",
      "Minibatch accuracy: 80.9%\n",
      "Validation accuracy: 78.5%\n",
      "Minibatch loss at step 17500: 3.682508\n",
      "Minibatch accuracy: 77.8%\n",
      "Validation accuracy: 78.3%\n",
      "Minibatch loss at step 18000: 4.143695\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 78.5%\n",
      "Minibatch loss at step 18500: 3.219319\n",
      "Minibatch accuracy: 80.9%\n",
      "Validation accuracy: 78.6%\n",
      "Minibatch loss at step 19000: 2.934390\n",
      "Minibatch accuracy: 81.9%\n",
      "Validation accuracy: 78.5%\n",
      "Minibatch loss at step 19500: 3.083267\n",
      "Minibatch accuracy: 81.6%\n",
      "Validation accuracy: 78.5%\n",
      "Minibatch loss at step 20000: 2.849832\n",
      "Minibatch accuracy: 79.7%\n",
      "Validation accuracy: 78.6%\n",
      "Test accuracy: 81.1%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 20001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "#     writer = tf.train.SummaryWriter(\"logs/\", session.graph)\n",
    "    tf.initialize_all_variables().run()\n",
    "    print('Initialized')\n",
    "    for step in range(num_steps):\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :, :, :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "        _, l, predictions = session.run(\n",
    "          [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        if (step % 500 == 0):\n",
    "            print('Minibatch loss at step %d: %f' % (step, l))\n",
    "            print('Minibatch accuracy: %.1f%%' % accuracy(predictions, batch_labels[:,1:6]))\n",
    "            print('Validation accuracy: %.1f%%' % accuracy(valid_prediction.eval(), valid_labels[:,1:6]))\n",
    "    print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels[:,1:6]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1  1 10 10 10]\n",
      " [ 6 10 10 10 10]\n",
      " [ 3 10 10 10 10]\n",
      " [ 2  5  1 10 10]\n",
      " [ 5 10 10 10 10]\n",
      " [ 6  2 10 10 10]\n",
      " [ 4  5 10 10 10]\n",
      " [ 5  1 10 10 10]\n",
      " [ 2  7  4 10 10]\n",
      " [ 3 10 10 10 10]\n",
      " [ 1 10 10 10 10]\n",
      " [ 1  6  7 10 10]\n",
      " [ 4 10 10 10 10]\n",
      " [ 1  5 10 10 10]\n",
      " [ 2 10 10 10 10]\n",
      " [ 4 10 10 10 10]\n",
      " [ 8  2 10 10 10]\n",
      " [ 1  4 10 10 10]\n",
      " [ 6 10 10 10 10]\n",
      " [ 1  5 10 10 10]\n",
      " [ 1  8  8 10 10]\n",
      " [ 5 10 10 10 10]\n",
      " [ 1  5 10 10 10]\n",
      " [ 1  3 10 10 10]\n",
      " [ 5  2  3 10 10]\n",
      " [ 3  2 10 10 10]\n",
      " [ 1  7 10 10 10]\n",
      " [ 1 10 10 10 10]\n",
      " [ 5 10 10 10 10]\n",
      " [ 2 10 10 10 10]\n",
      " [ 1  4 10 10 10]\n",
      " [ 3  2 10 10 10]\n",
      " [ 1  6 10 10 10]\n",
      " [ 2 10 10 10 10]\n",
      " [ 2  5  7 10 10]\n",
      " [ 5 10 10 10 10]\n",
      " [ 2 10 10 10 10]\n",
      " [ 4  3 10 10 10]\n",
      " [ 5 10 10 10 10]\n",
      " [ 1  6 10 10 10]\n",
      " [ 7  1 10 10 10]\n",
      " [ 9  9 10 10 10]\n",
      " [ 1  3 10 10 10]\n",
      " [ 7 10 10 10 10]\n",
      " [ 1 10 10 10 10]\n",
      " [ 3  9 10 10 10]\n",
      " [ 5  3 10 10 10]\n",
      " [ 1 10 10 10 10]\n",
      " [ 1  8 10 10 10]\n",
      " [ 6 10 10 10 10]\n",
      " [ 3  9 10 10 10]\n",
      " [ 4  5 10 10 10]\n",
      " [ 1  9 10 10 10]\n",
      " [ 6  3 10 10 10]\n",
      " [ 3  2 10 10 10]\n",
      " [ 1 10 10 10 10]\n",
      " [ 7 10 10 10 10]\n",
      " [ 5  1 10 10 10]\n",
      " [ 3  1 10 10 10]\n",
      " [ 3  1 10 10 10]\n",
      " [ 3  2 10 10 10]\n",
      " [ 4 10 10 10 10]\n",
      " [ 2  4 10 10 10]\n",
      " [ 1 10 10 10 10]]\n"
     ]
    }
   ],
   "source": [
    "print(np.argmax(predictions, 2).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "patch_size = 5\n",
    "depth1 = 16\n",
    "depth2 = 32\n",
    "depth3 = 64\n",
    "num_hidden = 1024\n",
    "\n",
    "image_size = 32\n",
    "num_labels = 11 # 0-9, + blank \n",
    "num_channels = 1 # grayscale\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "\n",
    "    def weight_varible(shape):\n",
    "        initial = tf.truncated_normal(shape, stddev = 0.1)\n",
    "        return tf.Variable(initial)\n",
    "\n",
    "    def bias_varible(shape):\n",
    "        initial = tf.constant(1.0, shape = shape)\n",
    "        return tf.Variable(initial)\n",
    "    \n",
    "    def conv2d(data, weight):\n",
    "        # strides [1, x_movement, y_movement, 1]\n",
    "        return tf.nn.conv2d(data, weight, strides = [1, 1, 1, 1], padding = 'SAME')\n",
    "\n",
    "    def max_pooling(data):\n",
    "        return tf.nn.max_pool(data, ksize = [1, 2, 2, 1], strides = [1, 2, 2, 1], padding = 'SAME')\n",
    "    \n",
    "#     def get_class_wb():\n",
    "#         weights = tf.Variable(tf.truncated_normal([num_hidden, num_labels]))\n",
    "#         biases = tf.Variable(tf.constant(1.0, shape=[num_labels]))\n",
    "#         return weights, biases #, logits\n",
    "    \n",
    "    # Input data.\n",
    "    tf_train_dataset = tf.placeholder(\n",
    "    tf.float32, shape=(batch_size, image_size, image_size, num_channels))\n",
    "    tf_train_labels = tf.placeholder(tf.int32, shape=(batch_size, 6))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    beta_regul = tf.placeholder(tf.float32)\n",
    "    \n",
    "    \n",
    "    # Varibles\n",
    "    # conv1 layer 1\n",
    "#     layer1_weights = weight_varible([patch_size, patch_size, num_channels, depth1])\n",
    "    layer1_weights = tf.get_variable('W1',shape=[patch_size, patch_size, num_channels, depth1],\\\n",
    "                                     initializer=tf.contrib.layers.xavier_initializer_conv2d())\n",
    "    layer1_biases = bias_varible([depth1]) # 16\n",
    "    # conv2 layer 2\n",
    "#     layer2_weights = weight_varible([patch_size, patch_size, depth1, depth2]) # in depth1, out depth2\n",
    "    layer2_weights = tf.get_variable('W2',shape=[patch_size, patch_size, depth1, depth2],\\\n",
    "                                     initializer=tf.contrib.layers.xavier_initializer_conv2d())\n",
    "    layer2_biases = bias_varible([depth2]) # 32\n",
    "    # conv3 layer 3\n",
    "#     layer3_weights = weight_varible([patch_size, patch_size, depth2, depth3]) # in depth2, out depth3\n",
    "    layer3_weights = tf.get_variable('W3',shape=[patch_size, patch_size, depth2, depth3],\\\n",
    "                                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "    layer3_biases = bias_varible([depth3]) # 64\n",
    "    \n",
    "    # func1 layer 4\n",
    "#     layer4_weights = weight_varible([image_size // 32 * image_size // 32 * depth3, num_hidden])\n",
    "    layer4_weights = tf.get_variable('W4',shape=[image_size // 8 * image_size // 8 * depth3, num_hidden],\\\n",
    "                                     initializer=tf.contrib.layers.xavier_initializer_conv2d())\n",
    "    layer4_biases = bias_varible([num_hidden])\n",
    "\n",
    "    s1_w = tf.get_variable(\"WS1\", shape=[num_hidden, num_labels],\\\n",
    "           initializer=tf.contrib.layers.xavier_initializer())\n",
    "    s1_b = tf.Variable(tf.constant(1.0, shape=[num_labels]), name='BS1')\n",
    "    s2_w = tf.get_variable(\"WS2\", shape=[num_hidden, num_labels],\\\n",
    "           initializer=tf.contrib.layers.xavier_initializer())\n",
    "    s2_b = tf.Variable(tf.constant(1.0, shape=[num_labels]), name='BS2')\n",
    "    s3_w = tf.get_variable(\"WS3\", shape=[num_hidden, num_labels],\\\n",
    "           initializer=tf.contrib.layers.xavier_initializer())\n",
    "    s3_b = tf.Variable(tf.constant(1.0, shape=[num_labels]), name='BS3')\n",
    "    s4_w = tf.get_variable(\"WS4\", shape=[num_hidden, num_labels],\\\n",
    "           initializer=tf.contrib.layers.xavier_initializer())\n",
    "    s4_b = tf.Variable(tf.constant(1.0, shape=[num_labels]), name='BS4')\n",
    "    s5_w = tf.get_variable(\"WS5\", shape=[num_hidden, num_labels],\\\n",
    "           initializer=tf.contrib.layers.xavier_initializer())\n",
    "    s5_b = tf.Variable(tf.constant(1.0, shape=[num_labels]), name='BS5')\n",
    "    \n",
    "    sw = [s1_w, s2_w, s3_w, s4_w, s5_w]\n",
    "    \n",
    "    global_step = tf.Variable(0)  # count the number of steps taken.\n",
    "    \n",
    "    def model(dataset, keep_prob):\n",
    "        # conv1 layer 1\n",
    "        hidden1 = tf.nn.relu(conv2d(dataset, layer1_weights) + layer1_biases) # 28 * 28 * depth1\n",
    "        pool1 = max_pooling(hidden1) # 14 * 14 * depth1\n",
    "        # conv2 layer 2\n",
    "        hidden2 = tf.nn.relu(conv2d(pool1, layer2_weights) + layer2_biases) # 10 * 10 * depth2\n",
    "        pool2 = max_pooling(hidden2) # 5 * 5 * depth2\n",
    "        # conv3 layer 3\n",
    "        hidden3 = tf.nn.relu(conv2d(pool2, layer3_weights) + layer3_biases) # 1 * 1 * depth3\n",
    "        pool3 = max_pooling(hidden3) # 1 * 1 * depth3\n",
    "        \n",
    "        shape = pool3.get_shape().as_list()\n",
    "        pool3_flat = tf.reshape(pool3, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "        \n",
    "#         # func1 layer 4\n",
    "        hidden4 = tf.nn.relu(tf.matmul(pool3_flat, layer4_weights) + layer4_biases)\n",
    "        hidden4_drop = tf.nn.dropout(hidden4, keep_prob)\n",
    "\n",
    "        logits_1 = tf.matmul(hidden4_drop, s1_w) + s1_b\n",
    "        logits_2 = tf.matmul(hidden4_drop, s2_w) + s2_b\n",
    "        logits_3 = tf.matmul(hidden4_drop, s3_w) + s3_b\n",
    "        logits_4 = tf.matmul(hidden4_drop, s4_w) + s4_b\n",
    "        logits_5 = tf.matmul(hidden4_drop, s5_w) + s5_b\n",
    "        \n",
    "        return [logits_1, logits_2, logits_3, logits_4, logits_5]\n",
    "    \n",
    "# Training computation.\n",
    "#     logits = model(tf_train_dataset)\n",
    "    [logits_1, logits_2, logits_3, logits_4, logits_5] = model(tf_train_dataset, 0.9375)\n",
    "    \n",
    "#     loss_per_digit = [tf.reduce_mean(\n",
    "#                         tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "#                             logits[i],\n",
    "#                             tf_train_labels[:,i+1]\n",
    "#                         )) + beta_regul * tf.nn.l2_loss(sw[i])\n",
    "#                        for i in range(5)]\n",
    "    \n",
    "#     loss = tf.add_n(loss_per_digit)\n",
    "    loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits_1, tf_train_labels[:,1])) +\\\n",
    "tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits_2, tf_train_labels[:,2])) +\\\n",
    "tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits_3, tf_train_labels[:,3])) +\\\n",
    "tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits_4, tf_train_labels[:,4])) +\\\n",
    "tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits_5, tf_train_labels[:,5]))\n",
    "\n",
    "\n",
    "    # Optimizer.\n",
    "    learning_rate = tf.train.exponential_decay(0.05, global_step, 1000, 0.70, staircase=True)\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "    \n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.pack([tf.nn.softmax(model(tf_train_dataset, 1.0)[0]),\\\n",
    "                      tf.nn.softmax(model(tf_train_dataset, 1.0)[1]),\\\n",
    "                      tf.nn.softmax(model(tf_train_dataset, 1.0)[2]),\\\n",
    "                      tf.nn.softmax(model(tf_train_dataset, 1.0)[3]),\\\n",
    "                      tf.nn.softmax(model(tf_train_dataset, 1.0)[4])])\n",
    "#     train_prediction = tf.pack([tf.nn.softmax(logits[i]) for i in range(5)])\n",
    "    valid_prediction = tf.pack([tf.nn.softmax(model(tf_valid_dataset, 1.0)[0]),\\\n",
    "                      tf.nn.softmax(model(tf_valid_dataset, 1.0)[1]),\\\n",
    "                      tf.nn.softmax(model(tf_valid_dataset, 1.0)[2]),\\\n",
    "                      tf.nn.softmax(model(tf_valid_dataset, 1.0)[3]),\\\n",
    "                      tf.nn.softmax(model(tf_valid_dataset, 1.0)[4])])\n",
    "    test_prediction = tf.pack([tf.nn.softmax(model(tf_test_dataset, 1.0)[0]),\\\n",
    "                     tf.nn.softmax(model(tf_test_dataset, 1.0)[1]),\\\n",
    "                     tf.nn.softmax(model(tf_test_dataset, 1.0)[2]),\\\n",
    "                     tf.nn.softmax(model(tf_test_dataset, 1.0)[3]),\\\n",
    "                     tf.nn.softmax(model(tf_test_dataset, 1.0)[4])])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 24.621223\n",
      "Minibatch accuracy: 1.6%\n",
      "Validation accuracy: 62.7%\n",
      "Minibatch loss at step 500: 6.172132\n",
      "Minibatch accuracy: 61.6%\n",
      "Validation accuracy: 61.1%\n",
      "Minibatch loss at step 1000: 5.791682\n",
      "Minibatch accuracy: 65.3%\n",
      "Validation accuracy: 64.5%\n",
      "Minibatch loss at step 1500: 5.518498\n",
      "Minibatch accuracy: 67.2%\n",
      "Validation accuracy: 64.5%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-bef88d543fb8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mfeed_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mtf_train_dataset\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mbatch_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf_train_labels\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mbatch_labels\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         _, l, predictions = session.run(\n\u001b[0;32m---> 13\u001b[0;31m           [optimizer, loss, train_prediction], feed_dict=feed_dict)\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m500\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Minibatch loss at step %d: %f'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/RyanG/anaconda/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    708\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    709\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 710\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    711\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    712\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/RyanG/anaconda/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    906\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 908\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    909\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    910\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/RyanG/anaconda/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    956\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    957\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m--> 958\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m    959\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    960\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/Users/RyanG/anaconda/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m    963\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 965\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    966\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/RyanG/anaconda/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m    945\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m    946\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 947\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m    948\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_steps = 20001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "#     writer = tf.train.SummaryWriter(\"logs/\", session.graph)\n",
    "    tf.initialize_all_variables().run()\n",
    "    print('Initialized')\n",
    "    for step in range(num_steps):\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :, :, :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "        _, l, predictions = session.run(\n",
    "          [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        if (step % 500 == 0):\n",
    "            print('Minibatch loss at step %d: %f' % (step, l))\n",
    "            print('Minibatch accuracy: %.1f%%' % accuracy(predictions, batch_labels[:,1:6]))\n",
    "            print('Validation accuracy: %.1f%%' % accuracy(valid_prediction.eval(), valid_labels[:,1:6]))\n",
    "    print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels[:,1:6]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2  6 10 10 10]\n",
      " [ 1  4 10 10 10]\n",
      " [ 1  3 10 10 10]\n",
      " [ 1  1  1 10 10]\n",
      " [ 1  1  2 10 10]\n",
      " [ 6 10 10 10 10]\n",
      " [ 4 10 10 10 10]\n",
      " [ 3  2  7 10 10]\n",
      " [ 3  3 10 10 10]\n",
      " [ 1  3  8 10 10]\n",
      " [ 1  8 10 10 10]\n",
      " [ 2 10 10 10 10]\n",
      " [ 1  9  6 10 10]\n",
      " [ 2  1 10 10 10]\n",
      " [ 3  2 10 10 10]\n",
      " [ 3  7  2 10 10]\n",
      " [ 4  3 10 10 10]\n",
      " [ 1  5  6 10 10]\n",
      " [ 2  2 10 10 10]\n",
      " [ 2  9 10 10 10]\n",
      " [ 1  2  5 10 10]\n",
      " [ 6 10 10 10 10]\n",
      " [ 3  5 10 10 10]\n",
      " [ 4  3 10 10 10]\n",
      " [ 5  9  4 10 10]\n",
      " [ 1  3  6 10 10]\n",
      " [ 1  1 10 10 10]\n",
      " [ 1  2  5  7 10]\n",
      " [ 9  3 10 10 10]\n",
      " [ 4  2 10 10 10]\n",
      " [ 4  6 10 10 10]\n",
      " [ 5 10 10 10 10]\n",
      " [ 4  3 10 10 10]\n",
      " [ 2  7 10 10 10]\n",
      " [ 1 10 10 10 10]\n",
      " [ 2  5  7 10 10]\n",
      " [ 7 10 10 10 10]\n",
      " [ 4  4 10 10 10]\n",
      " [ 5  1 10 10 10]\n",
      " [ 1 10 10 10 10]\n",
      " [ 2  7 10 10 10]\n",
      " [ 5  8 10 10 10]\n",
      " [ 1  6  3 10 10]\n",
      " [ 4  1 10 10 10]\n",
      " [ 1  5  3 10 10]\n",
      " [ 1  2  4 10 10]\n",
      " [ 3 10  6 10 10]\n",
      " [ 2  2  9 10 10]\n",
      " [ 6  8 10 10 10]\n",
      " [ 3 10 10 10 10]\n",
      " [ 1  4 10 10 10]\n",
      " [ 4  6 10 10 10]\n",
      " [ 5  3 10 10 10]\n",
      " [ 5 10  2 10 10]\n",
      " [ 6  3  7 10 10]\n",
      " [ 2  4  8 10 10]\n",
      " [ 1  2 10 10 10]\n",
      " [ 3  2 10 10 10]\n",
      " [ 5  4 10 10 10]\n",
      " [ 1  1  2 10 10]\n",
      " [ 3  2 10 10 10]\n",
      " [ 5  7 10 10 10]\n",
      " [ 2  1 10 10 10]\n",
      " [ 8 10 10 10 10]]\n"
     ]
    }
   ],
   "source": [
    "print(batch_labels[:,1:6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1 10 10 10 10]\n",
      " [ 1 10 10 10 10]\n",
      " [ 1 10 10 10 10]\n",
      " [ 1 10 10 10 10]\n",
      " [ 1 10 10 10 10]\n",
      " [ 1 10 10 10 10]\n",
      " [ 1 10 10 10 10]\n",
      " [ 1 10 10 10 10]\n",
      " [ 1 10 10 10 10]\n",
      " [ 1 10 10 10 10]\n",
      " [ 1 10 10 10 10]\n",
      " [ 1 10 10 10 10]\n",
      " [ 1 10 10 10 10]\n",
      " [ 1 10 10 10 10]\n",
      " [ 1 10 10 10 10]\n",
      " [ 1 10 10 10 10]\n",
      " [ 1 10 10 10 10]\n",
      " [ 1 10 10 10 10]\n",
      " [ 1 10 10 10 10]\n",
      " [ 1 10 10 10 10]\n",
      " [ 1 10 10 10 10]\n",
      " [ 1 10 10 10 10]\n",
      " [ 1 10 10 10 10]\n",
      " [ 1 10 10 10 10]\n",
      " [ 1 10 10 10 10]\n",
      " [ 1 10 10 10 10]\n",
      " [ 1 10 10 10 10]\n",
      " [ 1 10 10 10 10]\n",
      " [ 1 10 10 10 10]\n",
      " [ 1 10 10 10 10]\n",
      " [ 1 10 10 10 10]\n",
      " [ 1 10 10 10 10]\n",
      " [ 1 10 10 10 10]\n",
      " [ 1 10 10 10 10]\n",
      " [ 1 10 10 10 10]\n",
      " [ 1 10 10 10 10]\n",
      " [ 1 10 10 10 10]\n",
      " [ 1 10 10 10 10]\n",
      " [ 1 10 10 10 10]\n",
      " [ 1 10 10 10 10]\n",
      " [ 1 10 10 10 10]\n",
      " [ 1 10 10 10 10]\n",
      " [ 1 10 10 10 10]\n",
      " [ 1 10 10 10 10]\n",
      " [ 1 10 10 10 10]\n",
      " [ 1 10 10 10 10]\n",
      " [ 1 10 10 10 10]\n",
      " [ 1 10 10 10 10]\n",
      " [ 1 10 10 10 10]\n",
      " [ 1 10 10 10 10]\n",
      " [ 1 10 10 10 10]\n",
      " [ 1 10 10 10 10]\n",
      " [ 1 10 10 10 10]\n",
      " [ 1 10 10 10 10]\n",
      " [ 1 10 10 10 10]\n",
      " [ 1 10 10 10 10]\n",
      " [ 1 10 10 10 10]\n",
      " [ 1 10 10 10 10]\n",
      " [ 1 10 10 10 10]\n",
      " [ 1 10 10 10 10]\n",
      " [ 1 10 10 10 10]\n",
      " [ 1 10 10 10 10]\n",
      " [ 1 10 10 10 10]\n",
      " [ 1 10 10 10 10]]\n"
     ]
    }
   ],
   "source": [
    "print(np.argmax(predictions, 2).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
